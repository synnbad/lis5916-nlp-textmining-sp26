{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87aa4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c01ea50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\" This is the first document.\",\n",
    "             \" This document is the second document.\",  \n",
    "                \" And this is the third one.\",  \n",
    "                \" Is this the first document?\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4c27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ec698d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee8f0e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Document-term matrix:\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "print(\"Document-term matrix:\\n\", X.toarray())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e67cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe79b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This is the first document.\",\n",
    "             \"This document is the second document.\",               \n",
    "                \"And this is the third one.\",  \n",
    "                \"Is this the first document?\"]          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c4d79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8454a69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "TF-IDF matrix:\n",
      " [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(\"feature names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF matrix:\\n\", X_tfidf.toarray())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f988ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35ccf8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This is the first document.\",         \n",
    "             \"This document is the second document.\",               \n",
    "                \"And this is the third one.\",  \n",
    "                \"Is this the first document?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca03425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\saa24b\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a15e9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c9c19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78653f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation for 'document': [-5.3761393e-04  2.3459077e-04  5.1012170e-03  9.0115219e-03\n",
      " -9.3035055e-03 -7.1186870e-03  6.4577162e-03  8.9744031e-03\n",
      " -5.0161965e-03 -3.7644049e-03  7.3809391e-03 -1.5342169e-03\n",
      " -4.5370674e-03  6.5543531e-03 -4.8609949e-03 -1.8136933e-03\n",
      "  2.8776617e-03  9.8915887e-04 -8.2834894e-03 -9.4506554e-03\n",
      "  7.3119737e-03  5.0714435e-03  6.7562792e-03  7.6230383e-04\n",
      "  6.3530928e-03 -3.4065295e-03 -9.4848091e-04  5.7711215e-03\n",
      " -7.5222286e-03 -3.9373739e-03 -7.5092558e-03 -9.2885981e-04\n",
      "  9.5392875e-03 -7.3166536e-03 -2.3360765e-03 -1.9363161e-03\n",
      "  8.0779977e-03 -5.9297686e-03  4.5617318e-05 -4.7524953e-03\n",
      " -9.6023204e-03  5.0089518e-03 -8.7604597e-03 -4.3930719e-03\n",
      " -3.5214103e-05 -2.9548592e-04 -7.6621324e-03  9.6163880e-03\n",
      "  4.9832016e-03  9.2352722e-03 -8.1572160e-03  4.4980138e-03\n",
      " -4.1374546e-03  8.2675234e-04  8.4996261e-03 -4.4643688e-03\n",
      "  4.5164214e-03 -6.7876368e-03 -3.5471660e-03  9.3982853e-03\n",
      " -1.5784105e-03  3.2352045e-04 -4.1381051e-03 -7.6831253e-03\n",
      " -1.5093960e-03  2.4685122e-03 -8.8934030e-04  5.5310265e-03\n",
      " -2.7425813e-03  2.2589052e-03  5.4565049e-03  8.3474834e-03\n",
      " -1.4548642e-03 -9.2107793e-03  4.3709916e-03  5.6996895e-04\n",
      "  7.4426048e-03 -8.1429747e-04 -2.6364601e-03 -8.7528294e-03\n",
      " -8.5558271e-04  2.8258313e-03  5.4009468e-03  7.0547434e-03\n",
      " -5.7027498e-03  1.8572190e-03  6.0867225e-03 -4.7976980e-03\n",
      " -3.1054933e-03  6.7991368e-03  1.6290357e-03  1.9226066e-04\n",
      "  3.4747359e-03  2.1982045e-04  9.6214656e-03  5.0585950e-03\n",
      " -8.9198640e-03 -7.0399828e-03  9.0400706e-04  6.3935039e-03]\n"
     ]
    }
   ],
   "source": [
    "vector_representation = model.wv['document']\n",
    "print(\"Vector representation for 'document':\", vector_representation)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d8637da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "056701ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = 'alexander_the_great.txt'  # Update with your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76e41add",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5698875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "sentences = text.split('\\n')  # Split into sentences based on newlines\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences if sentence.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a513cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8cd6f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('document', topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c8028a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 0.21651451289653778),\n",
       " ('one', 0.0928850993514061),\n",
       " ('second', 0.06285111606121063),\n",
       " ('third', 0.027207495644688606),\n",
       " ('?', 0.016134241595864296),\n",
       " ('the', -0.010920586995780468),\n",
       " ('.', -0.02779584936797619),\n",
       " ('is', -0.05229273810982704),\n",
       " ('first', -0.05990160256624222),\n",
       " ('this', -0.11168749630451202)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
